# Portfolio Wizard Fix - LLM Integration

## What Was Broken

The portfolio wizard had **all hardcoded text responses** instead of using the LLM to generate dynamic, conversational prompts.

Example:
```python
def _wizard_menu(self) -> str:
    return """â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸ’° **PORTFOLIO WIZARD** (3 positions)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
What would you like to do?
  **1** âžœ Add a new holding
  ...
"""  # Hardcoded!
```

## What Was Fixed

### 1. Created LLM Wizard Prompt Generator
**`backend/agents/finance_wizard_llm.py`**
- Generates dynamic prompts using Claude API
- Keeps workflow logic separate from text generation
- Has fallback templates if LLM unavailable

### 2. Updated Finance Agent Wizard Methods
**All wizard prompt methods now use LLM:**
- `_wizard_menu()` - Main menu with holdings count
- `_wizard_add_ticker()` - Ask for ticker
- `_wizard_add_shares()` - Ask for shares
- `_wizard_add_type()` - Ask for asset type
- `_wizard_edit_select()` - Show holdings to edit
- `_wizard_edit_shares()` - Ask for new shares
- `_wizard_remove_select()` - Show holdings to remove
- `_wizard_confirm_remove()` - Confirm removal
- `_wizard_confirm_clear()` - Confirm clear all
- `_wizard_view()` - View portfolio

### 3. Kept Process Logic Hardcoded
The **workflow remains structured**:
- Step transitions (menu â†’ add_ticker â†’ add_shares â†’ add_type)
- Validation (ticker format, positive numbers)
- Database operations (add/edit/remove holdings)
- State management

Only the **text prompts** are LLM-generated!

## How It Works Now

### Before (Hardcoded):
```python
def _wizard_menu(self) -> str:
    return f"""ðŸ’° **PORTFOLIO WIZARD** ({len(holdings)} positions)

What would you like to do?
  1 âžœ Add a new holding
  ...
â€” Mamadou ðŸ’°"""
```

### After (LLM-Powered):
```python
async def _wizard_menu(self) -> str:
    llm = get_llm_client()
    context = {"holdings_count": len(holdings)}

    # LLM generates prompt in Mamadou's voice
    return await generate_wizard_prompt("menu", context, llm)
```

The LLM receives:
```
Generate a portfolio wizard main menu.
Current state: 3 holdings in portfolio.
Options: 1. Add / 2. Edit / 3. Remove / 4. View / 5. Clear / 0. Exit
Make it welcoming and clear.
```

And responds naturally:
```
ðŸ’° **PORTFOLIO WIZARD** (3 positions)

Hey! Ready to manage your portfolio. What would you like to do?

  **1** âžœ Add a new holding
  **2** âžœ Edit existing holding
  **3** âžœ Remove a holding
  **4** âžœ View portfolio
  **5** âžœ Clear all holdings
  **0** âžœ Exit

Reply with a number (0-5).

â€” Mamadou ðŸ’°
```

## Benefits

1. **Natural Conversation**: Prompts feel more human and contextual
2. **Persona Consistency**: Mamadou's personality shines through
3. **Flexibility**: Easy to adjust tone without code changes
4. **Maintained Structure**: Workflow logic stays reliable and predictable

## Testing

```python
from backend.agents.finance import FinanceAgent

# Start wizard
agent = FinanceAgent()
prompt = agent.start_wizard()
print(prompt)

# Process input
response = agent.process_wizard("1")  # Add holding
print(response)

response = agent.process_wizard("AAPL")  # Ticker
print(response)

response = agent.process_wizard("100")  # Shares
print(response)
```

Each prompt will be dynamically generated by Claude in Mamadou's voice!

## Fallback

If LLM is unavailable, the wizard automatically falls back to simple templates:
```python
def _fallback_prompt(step, context):
    if step == "menu":
        return """ðŸ’° PORTFOLIO WIZARD
        1. Add holding
        2. Edit holding
        ...
        â€” Mamadou ðŸ’°"""
```

No crashes, just less personality.

## Configuration

To enable LLM prompts:
1. Set `ANTHROPIC_API_KEY` in `.env`
2. Install: `pip install anthropic`
3. Restart the backend

That's it! The wizard will automatically use LLM-generated prompts.

## Cost Optimization

Each wizard interaction costs ~$0.0002-0.0005 (with Opus).

To reduce costs:
- Edit `finance_wizard_llm.py` to use `claude-sonnet-4` or `claude-haiku-4`
- Reduce `max_tokens` from 400 to 200
- Adjust `temperature` from 0.7 to 0.3 for more consistent responses

## Summary

**Workflow**: Hardcoded âœ“
**Prompts**: LLM-Generated âœ“
**Personality**: Authentic âœ“
**Fallback**: Safe âœ“
